#+title: Diffusion
#+Property: header-args:python :session py :async nil :tangle diff.py :results output :comments org
#+name: imports
#+begin_src python
import torch as pt
import torch.nn as nn
from torch import Tensor
#+end_src

#+RESULTS:

* Discrete Noise Diffusion
fixed noise schedule used the same noise schedule as the continnous version using a fixed schedule sampled from \(\beta (t)\)
** Noise Schedule
\begin{align}
\label{eq:4}
x_t &= \lambda  x_0 + (1-\lambda ) \mathcal{N}(0,I)
\end{align}
where \(\lambda \) is the discrete pendant to the integral 

\begin{align}
\label{eq:5}
\overline{\alpha}_n &= \prod_{j=0}^n \alpha_{t}
\lambda &= 
\end{align}

\begin{align}
\label{eq:1}
x_t &= \lambda   x_0  + (1-\lambda  ) \mathcal{N}(0,I)
\end{align}
where \(\lambda \) is the noise schedule

\begin{align}
\label{eq:2}
\lambda &= e^{-\int_{0}^t \beta (t) \, \mathrm{d}t }
\end{align}
with a cosine noise schedule
\begin{align}
\label{eq:3}
\beta (t) &= \cos^2 \left( \frac{\pi }{2} \frac{t+\epsilon }{1 + \epsilon } \right)
\end{align}
#+begin_src python :tangle src/train_diffusion.py
import torch as pt
from dataclasses import dataclass
class NoiseSchedule:

    def __init__(self,dev : pt.device ,  eps = 0.008 , T=1000 , linear=False):
        # 1. Create time steps [0, T]
        # 2. Compute alpha_bar using cosine schedule
        
        self.T = T
        self.time = pt.linspace(0 , self.T , self.T+1).to(dev)
        alpha_bar = pt.cos((self.time / self.T + eps) / (1 + eps) * pt.pi / 2) ** 2
        alpha_bar = alpha_bar / alpha_bar[0]  # normalize so alpha_bar[0] = 1

        beta = 1 - (alpha_bar[1:] / alpha_bar[:-1])
        beta= pt.clip(beta, 0, 0.999)

        self.alpha_bar = alpha_bar[1:].to(dev)
        self.beta = beta.to(dev)
        if linear:
            self.beta = pt.linspace(1e-4 , 0.02 , self.T).to(dev)
            self.alpha_bar = pt.cumprod(1-self.beta, dim=0).to(dev)
        self.alpha = (1-self.beta).to(dev)

#+end_src

#+RESULTS:

#+name: alphabars.png
#+begin_src python :results file graphics output :file alphabars.png
import matplotlib.pyplot as plt
N = NoiseSchedule(dev)
plt.ioff()
plt.close("all")
plt.plot(N.alpha_bar.to('cpu') , label=r"$\bar{\alpha}_t$")
plt.plot(N.alpha.to('cpu') , label=r"$\alpha_t$")
plt.legend()
#+end_src

#+RESULTS: alphabars.png
[[file:alphabars.png]]
** Forward Procces
#+begin_src python :tangle src/train_diffusion.py
import torch as pt
import torch.nn as nn
from torch import Tensor
    
def diffusion_step(x : Tensor , model : nn.Module,dev, N):
    """
    data : minibatch with label and input data:
        data[0].shape = (Batchsize , channels , xsize , ysize )
    model: trainings model for example uNet
    """
    index = pt.randint(0, len(N.beta) , (x.size(0),) , device=dev)
    
    noise = pt.randn_like(x, device=dev)
    noisy_input = pt.sqrt(1-N.alpha_bar[index]).view(-1,1,1,1) *  noise + pt.sqrt(N.alpha_bar[index]).view(-1,1,1,1) * x
    
    
    output = model(noisy_input, N.time[index]) 
    return output , noise

#+end_src

#+RESULTS:

** Backward Procces




#+begin_src python :tangle src/train_diffusion.py 
import torch as pt
import torch.nn as nn
from torch import Tensor
import torchvision

def log_samples(writer, x, step, nrow=8):
    """
    writer: TensorBoard SummaryWriter
    x: tensor (B,C,H,W) with values in [-1,1]
    step: current iteration / timestep
    nrow: images per row
    """
    # normalize to [0,1]
    x_norm = (x + 1) / 2
    grid = torchvision.utils.make_grid(x_norm, nrow=nrow)
    writer.add_image("samples", grid, global_step=step)

def sample_diffusion(T:int,model: nn.Module , shape:tuple , dev:pt.device , N , writer=None):
   x = pt.randn(shape , device=dev)
   for t in reversed(range(0,T)):
      
      with pt.no_grad():
         predicted_noise = model(x,N.time[None,t])
      #sigma = pt.sqrt(
      #   N.beta[t] * (1 - N.alpha_bar[t-1]) / (1 - N.alpha_bar[t])
      #)
      weighted_noise = (N.beta[t].view(-1,1,1,1)/pt.sqrt(1.-N.alpha_bar[t])) * predicted_noise
      x = (1/pt.sqrt(N.alpha[t]).view(-1,1,1,1)) * (x - weighted_noise)
      if t>0:
          sigma = pt.sqrt(N.beta[t]* (1-N.alpha_bar[t-1])/(1-N.alpha_bar[t]))
          noise = pt.randn(shape , device=dev)
          x += sigma * noise
      #x = x.clamp(-1,1)

      if writer is not None and t%(T//10)==0:
          log_samples(writer, x, step=T-t)


   return x
#+end_src

#+RESULTS:



* Diffusion training structure

#+begin_src python :tangle src/train_diffusion.py
from tqdm import trange , tqdm
import torch as pt
from torch.utils.data import DataLoader
from torch.nn import Module , MSELoss
from torch.utils.tensorboard import SummaryWriter
from torch.optim import Optimizer, AdamW
from pathlib import Path

def train(
    model: Module,
    epochs: int,
    train_data: DataLoader,
    validation_data: DataLoader,
    writer: SummaryWriter,
    dev,
    N
):
    optimizer = AdamW(model.parameters(), lr=1e-4)
    Path("models_diffusion").mkdir(parents=True, exist_ok=True)
    Path("optim_diffusion").mkdir(parents=True, exist_ok=True)
    if Path("models_diffusion/checkpoint").exists():
        model.load_state_dict(pt.load("models_diffusion/checkpoint"))
    if Path("optim_diffusion/checkpoint").exists():
        optimizer.load_state_dict(pt.load("optim_diffusion/checkpoint"))
    loss_fn = MSELoss()
    min_loss = 10000.
    for e in trange(epochs):
        
        model.train(True)
        running_loss :float = 0.0
        for i,data in tqdm(enumerate(train_data) , total=len(train_data)):
            x , _ = data
            x=x.to(dev)
            optimizer.zero_grad()
            output , noise = diffusion_step(x , model , dev , N)

            loss = loss_fn(noise , output)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            if i% 100 == 99:
                last_loss = running_loss / 100  # loss per batch
                tb_x = e * len(data) + i + 1
                running_loss = 0.0

        writer.add_scalar("Loss/train", last_loss, e)

        validation_loss = 0.
        model.eval()
        for i , data in enumerate(validation_data):
            with pt.no_grad():
                input, label = data
                input=input.to(dev)
                output , noise = diffusion_step(input , model, dev , N)
                loss = loss_fn(noise , output)
                validation_loss += loss.item()
        validation_loss /= len(validation_data)
        if validation_loss < min_loss :
            pt.save(model.state_dict() , f"models_diffusion/checkpoint")
            pt.save(optimizer.state_dict() , f"optim_diffusion/checkpoint")
            min_loss= validation_loss
        writer.add_scalar("Loss/val", validation_loss, e)
        if e%10 ==0:
            sample_diffusion(N.T , model ,  (8,3,64,64) , dev , N , writer=writer)
    
#+end_src

#+RESULTS:

* Time Embeddings
#+RESULTS:
#+begin_src python :tangle src/embedding.py
from torch.nn import Module , Sequential
from torch import Tensor
import torch.nn as nn
import torch as pt

class SinusoidalEmbedding(nn.Module):
    def __init__(self, dimension: int , hidden : int , out:int):
        super().__init__()
        self.dim: int = dimension
        self.net = nn.Sequential(
            nn.Linear(self.dim * 2 , hidden ),
            nn.ReLU(),
            nn.Linear(hidden , out )
        )


    def embedd(self, t:Tensor):
        # Create embedding frequencies
        i = pt.arange(self.dim, device=t.device)
        freqs = 10000 ** (-2 * i / self.dim)
        # Compute sin and cos embeddings
        embedded_sin = pt.sin(t[:, None] * freqs[None, :])
        embedded_cos = pt.cos(t[:, None] * freqs[None, :])
        return pt.cat([embedded_sin, embedded_cos], dim=1)

    def forward(self, t):
        input_tensor = self.embedd(t)
        output_tensor = self.net(input_tensor)
        return output_tensor
#+end_src

#+RESULTS:

* Attention
:PROPERTIES:
:header-args:python: :tangle src/attention.py :session
:END:
See [[https://www.digitalocean.com/community/tutorials/attention-mechanisms-in-computer-vision-cbam][Attention in CNN Digitalocean]].
Aim of spatial attention is t capture local features, this is done using dot product similarity un three feature maps \(Q,K,V\) by which we generate using a convolutional layer
** Scaled Dot Attention

#+begin_src python
from torch import Tensor
from torch import nn
from typing import Tuple
import torch as pt
from torch import nn

class MultiheadAttention(nn.Module):
    def __init__(self , in_ch , out_ch , heads):
        super().__init__()
        self.ch = in_ch
        # ensure propper divisibility 
        self.dim = in_ch // heads
        assert self.dim != 0
        self.heads = heads 
        self.feature_map = nn.Conv2d(in_ch , self.dim * self.heads * 3 , kernel_size=1)
        self.softmax = nn.Softmax(dim=-1)
        self.proj = nn.Conv2d(self.heads * self.dim, in_ch, kernel_size=1)
        self.norm = nn.GroupNorm(1,in_ch)
        return None

    def forward(self , x:Tensor):
        B , C , H , W = x.shape
        x = self.norm(x)
        
        QKV : Tuple[Tensor,Tensor,Tensor]  = self.feature_map(x).view(B,  3 * self.heads , self.dim ,-1).chunk(3 , dim=1) # type: tuple[Tensor, Tensor, Tensor]
        Q,K,V = QKV
        Q:Tensor =Q.permute(0,1,3,2)# B,Heads , HW , dim
        K:Tensor =K.permute(0,1,2,3)# B,Heads , dim , HW
        V:Tensor =V.permute(0,1,3,2)# B,Heads , HW, dim

        dot : Tensor= Q @ K # B , Heads , HW , HW
        attention: Tensor = self.softmax(dot*C**-0.5)
        values = attention @ V # B , Heads , HW , dim
        values = values.permute(0,1,3,2)
        values = values.reshape(B,self.heads * self.dim , H,W)
        return self.proj(values)
        #return self.proj(values)
#+end_src

#+RESULTS:
: None

** Spatial Attention

#+begin_src python
from torch import nn
from torch import Tensor
class ChannelAttention(nn.Module):
    def __init__(self , channels):
        super().__init__()
        self.mlp = nn.Sequential(
            nn.Conv2d(channel , channels//2, 1),
            nn.SiLU(),
            nn.Conv2d(channels // 2 , channels ,1),
        )
        self.meanpool = nn.AdaptiveAvgPool2d(1)
        self.maxpool = nn.AdaptiveMaxPool2d(1)
        self.actv = Sigmoid()

    def forward(self,x:Tensor):
        mean_attention = self.mlp(self.meanpool(x))
        max_attention = self.mlp(self.maxpool(x))
        attention = self.actv(max_attention + mean_attention)
        return x * attention


class SpatialAttention(nn.Module):
    def __init__(self , channels):
        super().__init__()
        kernel_size=7
        self.conv = nn.Sequential(
            nn.Conv2d(2 ,1 , kernel_size , padding=(kernel_size-1)//2)
            nn.BatchNorm2d(1),
        )
        self.actv = nn.Sigmoid()

        
    def forward(self,x:Tensor):
        mean_pool = pt.mean(x,dim=1 , keepdims=True)
        max_pool = pt.max(x,dim=1 , keepdims=True).values
        pool = pt.cat((mean_pool , max_pool) , dim=1)
        attention = self.actv(attention)
        return x * attention
#+end_src

#+RESULTS:

#+begin_src python

#+end_src
