#+title: Train
#+Property: header-args:python :session py :async nil :tangle eval.py

* Data and Setup
#+begin_src python :results output
import torch.nn as nn
import torch as pt
import matplotlib.pyplot as plt
import src.unet as unet
from importlib import reload
from torch.nn import init

plt.ioff()

reload(unet)
dev = pt.device("cpu")
if pt.xpu.is_available():
    print("Found Functional Intel GPU using dev=xpu")
    dev = pt.device("xpu")
if pt.cuda.is_available():
    print("Found Functional NVIDIA GPU using dev=cuda")
    dev = pt.device("cuda")
#+end_src

#+RESULTS:
: Q:torch.Size([5, 4, 64, 8])
: K:torch.Size([5, 4, 8, 64])
: V:torch.Size([5, 4, 64, 8])
: V:torch.Size([5, 4, 64, 8])
: torch.Size([5, 32, 8, 8])
: Found Functional NVIDIA GPU using dev=cuda







#+begin_src python 
uNet = unet.UNET(3, 3, 3, 5,  hidden_factor=50 , input_shape=(32,32)).to(dev)
uNet.init()
#+end_src

#+RESULTS:
: None





#+begin_src python
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader,Subset
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Resize((32,32)),
    transforms.Normalize((0.5,), (0.5,))])
# Create datasets for training & validation, download if necessary
training_set = torchvision.datasets.CIFAR10('./data', train=True, transform=transform, download=True )
validation_set = torchvision.datasets.CIFAR10('./data', train=False, transform=transform, download=True)
train_loader = DataLoader(training_set , batch_size = 128 , shuffle=True)
val_loader = DataLoader(validation_set , batch_size = 128 , shuffle=False)

#+end_src

#+RESULTS:
: None

* Autoencoder Training

#+begin_src python :tangle nil
from tqdm import trange , tqdm
import torch as pt
from torch.utils.data import DataLoader
from torch.nn import Module , MSELoss
from torch.utils.tensorboard import SummaryWriter
from torch.optim import Optimizer, AdamW
from pathlib import Path

def train(
    model: Module,
    epochs: int,
    train_data: DataLoader,
    validation_data: DataLoader,
    writer: SummaryWriter,
    dev,
):

    # ensure storage paths exist
    Path("models").mkdir(parents=True , exist_ok=True)
    Path("optim").mkdir(parents=True , exist_ok=True)
    
    optimizer = AdamW(model.parameters(), lr=1e-4)
    if Path("models/checkpoint").exists():
        print(f"Found Model Checkpoint. Loading Checkpoint")
        model.load_state_dict(pt.load("models/checkpoint"))
    if Path("optim/checkpoint").exists():
        print(f"Found Optimizer Checkpoint. Loading Checkpoint")
        optimizer.load_state_dict(pt.load("optim/checkpoint"))
    loss_fn = MSELoss()
    min_loss = 10000.
    for e in trange(epochs):
        
        model.train(True)
        running_loss :float = 0.0
        for i,data in tqdm(enumerate(train_data) , total=len(train_data)):
            x , label = data
            x=x.to(dev)
            optimizer.zero_grad()
            prediction = model(x)

            loss = loss_fn(prediction , label)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()
            if i% 100 == 99:
                last_loss = running_loss / (i+1)  # loss per batch
                tb_x = e * len(data) + i + 1
                running_loss = 0.0

        writer.add_scalar("Loss/train", last_loss, e)

        validation_loss = 0.
        for i , data in enumerate(validation_data):
            with pt.no_grad():
                input, label = data
                input=input.to(dev)
                validation_loss += loss.item()
        validation_loss /= len(validation_data)
        if validation_loss < min_loss :
            pt.save(model.state_dict() , f"models/checkpoint")
            pt.save(optimizer.state_dict() , f"optim/checkpoint")
            min_loss= validation_loss
        writer.add_scalar("Loss/val", validation_loss, e)
        if e%10 ==0:
            input , output = next(iter(validation_data))
            writer.add_image("input" , input[0],e )
            writer.add_image("output" , output[0],e )
            
    
#+end_src

#+RESULTS:
: None

#+begin_src python :results output :tangle nil
import training
from training import train_step , train
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

reload(training)

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))
train(uNet , 10 , debug_data , debug_data , writer, dev)
#+end_src

#+RESULTS:
: cf12ddbf-19fd-42e8-9f1a-e00d4eaa7262

* Diffusion Model Training

#+begin_src python :eval never
from src.train_diffusion import train , NoiseSchedule
import src.train_diffusion as tdf
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter
reload(tdf)
validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)
timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))
N = NoiseSchedule(dev)
train(uNet , 1000 , train_loader , val_loader, writer, dev , N)

#+end_src

#+RESULTS:
: /tmp/babel-tkRk3K/python-8kn08t

#+name: fig:images/sample
#+begin_src python :results graphics file output :file images/sample.png
import matplotlib.pyplot as plt
import random
plt.imshow(validation_set[random.randint(0,len(validation_set))][0].permute(1,2,0) , cmap="Greys" , interpolation='bicubic')
#+end_src

    #+RESULTS: fig:images/sample
    [[file:images/sample.png]]


#+begin_src python :results file graphics :file images/output.png :tangle img.py :sync

from src.train_diffusion import sample_diffusion
import src.unet as unet
from src.unet import UNET
import torch as pt
import matplotlib.pyplot as plt
from src.train_diffusion import   NoiseSchedule
import src.train_diffusion as tdf
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader,Subset
from importlib import reload

reload(unet)
transform = transforms.Compose(
    [transforms.ToTensor(),
    transforms.Resize((32,32)),
    transforms.Normalize((0.5,), (0.5,))])
plt.close('all')
plt.ioff()

dev = 'cuda'
    
uNet = UNET(3, 3, 3, 5,  hidden_factor=50 , input_shape=(64,64)).to(dev)
uNet.load_state_dict(pt.load("models_diffusion/checkpoint"))

reload(tdf)
N = NoiseSchedule(dev,T=1000)
shape = (1,3,32,32)
y = sample_diffusion(1000 , uNet ,shape , dev , N)
img = y[0].to("cpu").detach().permute(1,2,0).squeeze()
img_rescaled = (img - img.min()) / (img.max() - img.min())

fig , ax = plt.subplots(1,1 ,figsize=(5,4))
#ax[0].imshow(input[0,:,:].to("cpu").detach().squeeze() , cmap="Greys")
#ax[0].set_title("Original")
ax.imshow( img*0.5 + 0.5, cmap="Greys" , interpolation='bicubic')
ax.set_title(f"Prediction: min={img.min():.2f} , max={img.max():.2f}")


    #ax[2].imshow((input.to(dev)-y)[0,0,:,:]>>>.to("cpu").detach().squeeze() , cmap="gray")
    #ax[2].set_title("Difference")
for i in range(1):

    ax.set_xlabel("x")
    ax.set_ylabel("y")
fig.savefig("images/output.png")
fig
#+end_src

    #+RESULTS:
    [[file:images/output.png]]
* 
