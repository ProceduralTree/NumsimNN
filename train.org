#+title: Train
#+Property: header-args:python :session py :async nil :tangle src/eval.py

#+begin_src python :results output
import torch.nn as nn
import torch as pt
import matplotlib.pyplot as plt
import unet
from importlib import reload
from torch.nn import init

plt.ioff()

reload(unet)
dev = "cpu"
if pt.xpu.is_available():
    print("Found Functional Intel GPU using dev=xpu")
    dev = "xpu"
#+end_src

#+RESULTS:
: Found Functional Intel GPU using dev=xpu







#+begin_src python 
uNet = unet.UNET(1, 1, 4, 3, 1).to(dev)
uNet.init()
x = pt.rand(1, 1, 128, 128).to(dev)
x
#+end_src

#+RESULTS:
: tensor([[[[0.4868, 0.1529, 0.1093,  ..., 0.5392, 0.1307, 0.1845],
:           [0.3820, 0.3577, 0.5093,  ..., 0.8351, 0.6547, 0.6137],
:           [0.0819, 0.1998, 0.4208,  ..., 0.6519, 0.1354, 0.7123],
:           ...,
:           [0.7158, 0.9748, 0.2869,  ..., 0.0879, 0.7371, 0.7230],
:           [0.5710, 0.9781, 0.6428,  ..., 0.2320, 0.7880, 0.8724],
:           [0.5278, 0.8889, 0.4154,  ..., 0.6512, 0.6754, 0.7924]]]],
:        device='xpu:0')

#+begin_src python :results file graphics :file images/input.png
fig , ax = plt.subplots()
ax.imshow(x[0,0,:,:].to("cpu").detach().squeeze())
ax.set_xlabel("x")
ax.set_ylabel("y")
fig
#+end_src

#+RESULTS:
[[file:images/input.png]]

#+begin_src python :results file graphics :file images/output.png
plt.close('all')
input, output = validation_set[pt.randint(0, 1000, ())]
y = uNet(input.unsqueeze(1).to(dev))
fig , ax = plt.subplots(1,3 ,figsize=(9,3))
ax[0].imshow(input[0,:,:].to("cpu").detach().squeeze() , cmap="Greys")
ax[0].set_title("Original")
ax[1].imshow(y[0,0,:,:].to("cpu").detach().squeeze() , cmap="Greys")
ax[1].set_title("Prediction")
ax[2].imshow((input.to(dev)-y)[0,0,:,:].to("cpu").detach().squeeze() , cmap="gray")
ax[2].set_title("Difference")
for i in range(3):
     ax[i].set_xlabel("x")
     ax[i].set_ylabel("y")
fig
#+end_src

#+RESULTS:
[[file:images/output.png]]
#+begin_src python
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Resize((64,64)),
    transforms.Normalize((0.5,), (0.5,))])
# Create datasets for training & validation, download if necessary
training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True )
validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)
train_loader = DataLoader(training_set , batch_size = 128 , shuffle=True)
val_loader = DataLoader(validation_set , batch_size = 128 , shuffle=False)

#+end_src

#+RESULTS:
: None




#+begin_src python :results output
import training
from training import train_step , train
from datetime import datetime
from torch.utils.tensorboard import SummaryWriter

reload(training)

timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))
train(uNet , 10 , train_loader , val_loader , writer, dev)
#+end_src

#+RESULTS:
#+begin_example
  batch 100 loss: 6.363838259130716e-05
  batch 200 loss: 5.3746827878057954e-05
  batch 300 loss: 5.315183204947971e-05
  batch 400 loss: 5.049261602107435e-05
  batch 100 loss: 4.583166551310569e-05
  batch 200 loss: 4.3187630886677655e-05
  batch 300 loss: 4.0536627697292716e-05
  batch 400 loss: 3.8674877127050424e-05
  batch 100 loss: 3.5202193568693476e-05
  batch 200 loss: 3.327640531642828e-05
  batch 300 loss: 3.117037104675546e-05
  batch 400 loss: 3.110671009926591e-05
  batch 100 loss: 2.8248060072655788e-05
  batch 200 loss: 2.695693672285415e-05
  batch 300 loss: 2.6226015834254212e-05
  batch 400 loss: 2.4997467640787362e-05
  batch 100 loss: 2.3769642881234178e-05
  batch 200 loss: 2.3493134882301092e-05
  batch 300 loss: 2.1967953129205852e-05
  batch 400 loss: 2.1474691733601502e-05
  batch 100 loss: 2.0868009247351437e-05
  batch 200 loss: 1.9575314014218746e-05
  batch 300 loss: 1.978198236611206e-05
  batch 400 loss: 1.937852887203917e-05
  batch 100 loss: 1.806835102615878e-05
  batch 200 loss: 1.762168497953098e-05
  batch 300 loss: 1.778074828325771e-05
  batch 400 loss: 1.6848268074681982e-05
  batch 100 loss: 1.609790565998992e-05
  batch 200 loss: 1.6039505673688835e-05
  batch 300 loss: 1.5080656688951421e-05
  batch 400 loss: 1.5266349029843695e-05
  batch 100 loss: 1.4512685302179307e-05
  batch 200 loss: 1.404821532923961e-05
  batch 300 loss: 1.397876015107613e-05
  batch 400 loss: 1.3864771630323958e-05
  batch 100 loss: 1.3588585417892318e-05
  batch 200 loss: 1.2928436452057213e-05
  batch 300 loss: 1.2701105639280285e-05
  batch 400 loss: 1.2897334709123243e-05
#+end_example
